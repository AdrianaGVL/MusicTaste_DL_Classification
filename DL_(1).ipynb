{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUuusoYphxEr"
      },
      "source": [
        "# Genre song classification\n",
        "In out database we have spectrograms and chomagrams from different song genres: Alternative, Classical, Dance, Pop, Rock and Techno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgU0YiU3hxEs"
      },
      "source": [
        "### Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrmWOehZo7Fs",
        "outputId": "c25f4efe-368e-4a8e-cf65-eb4c19ff0825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "# Append the directory to your python path using sys\n",
        "sys.path.append('/content/gdrive/MyDrive/dataset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "tdXfnesZhxEt"
      },
      "outputs": [],
      "source": [
        "# Import general purpose python libraries\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image # For handling the images\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "# Import different Keras functionalities\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Add, Activation\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import concatenate\n",
        "from keras.constraints import MaxNorm\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras import backend as K\n",
        "from keras import layers, models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from keras.applications import ResNet50, VGG16\n",
        "\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Import function to plot the confussion matrix\n",
        "#import plotcm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acQvmDp6hxEt"
      },
      "source": [
        "### Configuration parameters\n",
        "Configuration values of different parts of the solution. You should change some of them to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO5OSJcghxEu",
        "outputId": "0af11b83-0e8c-44f4-ee9d-120c8e8f334d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/dataset/training/spectrogram\n"
          ]
        }
      ],
      "source": [
        "# Randomize the initial network weights\n",
        "random_seed = True\n",
        "\n",
        "# Paths to where spectrograms of training, testing, and validation images are\n",
        "database_dir = '/content/gdrive/MyDrive/dataset'\n",
        "train_dir = '/content/gdrive/MyDrive/dataset/training/spectrogram'\n",
        "val_dir = '/content/gdrive/MyDrive/dataset/val/spectrogram'\n",
        "test_dir = '/content/gdrive/MyDrive/dataset/test/spectrogram'\n",
        "\n",
        "# Directory where to store weights of the model and results\n",
        "experiment_rootdir = \"results/\"\n",
        "# Create experiment directory if it does not exists\n",
        "if not os.path.exists(experiment_rootdir):\n",
        "    os.makedirs(experiment_rootdir)\n",
        "\n",
        "version = 'VGG16_20_1e-3'\n",
        "weights_path = f\"weights_{version}.h5\" # Name of the file to store the weights\n",
        "weights_file = Path(weights_path)\n",
        "\n",
        "# Output dimension (number of sublects in our problem)\n",
        "num_classes = 6\n",
        "\n",
        "# Name of each gesture of the database\n",
        "CLASSES = ['Alternative', 'Classical', 'Dance', 'Pop', 'Rock', 'Techno']\n",
        "\n",
        "# Parameters that characterize the images\n",
        "img_height = 480\n",
        "img_width = 640\n",
        "img_channels = 3 # although some images could be rgb, we work with grayscale images\n",
        "color_mode = 'rgb'\n",
        "\n",
        "print(train_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlekhinYlXrA"
      },
      "source": [
        "## Configuration of parameters and train, val and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jCmerTZldzN",
        "outputId": "3156732d-95dd-4118-808c-961f4cb568e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1186 files belonging to 6 classes.\n",
            "Found 204 files belonging to 6 classes.\n",
            "Found 213 files belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "# Parameters that configures the training process\n",
        "batch_size = 20 # Batch size\n",
        "epochs = 20 # Number of epochs. Set to 20 for this work\n",
        "initial_lr = 1e-3 # Learning rate\n",
        "\n",
        "# 1. Generate train dataset (ds) from directory of samples\n",
        "train_ds = image_dataset_from_directory(directory=train_dir,\n",
        "                                        label_mode = 'categorical',\n",
        "                                        class_names=CLASSES,\n",
        "                                        batch_size=batch_size,\n",
        "                                        color_mode=color_mode,\n",
        "                                        image_size=(img_width,img_height), shuffle=True)\n",
        "\n",
        "# 2. Generate validation dataset (ds) from directory of samples\n",
        "val_ds  = image_dataset_from_directory(directory=val_dir,\n",
        "                                       label_mode = 'categorical',\n",
        "                                       class_names=CLASSES,\n",
        "                                       batch_size=batch_size,\n",
        "                                       color_mode=color_mode,\n",
        "                                       image_size=(img_width,img_height))\n",
        "\n",
        "# 3. Generate test dataset (ds) from directory of samples\n",
        "test_ds = image_dataset_from_directory(directory=test_dir,\n",
        "                                       label_mode = 'categorical',\n",
        "                                       class_names=CLASSES,\n",
        "                                       batch_size=batch_size,\n",
        "                                       color_mode=color_mode,\n",
        "                                       image_size=(img_width,img_height),\n",
        "                                       shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx6J0q-tm0CO"
      },
      "source": [
        "## Function def for plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "zy99qgSFm30X"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy_loss(history):\n",
        "    # 1. Plot accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['accuracy'])  # Use 'accuracy' instead of 'categorical_accuracy'\n",
        "    plt.plot(history.history['val_accuracy'])  # Use 'val_accuracy' instead of 'val_categorical_accuracy'\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "    # Save the figure\n",
        "    fig_save_path = os.path.join(experiment_rootdir, f\"accuracy_{version}.png\")\n",
        "    plt.savefig(fig_save_path)\n",
        "\n",
        "    # Show figure\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Plot loss\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    fig_save_path = os.path.join(experiment_rootdir, f\"loss_{version}.png\")\n",
        "    plt.savefig(fig_save_path)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "MfN_ndTSo5YO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, predicted_classes):\n",
        "    cm = confusion_matrix(y_true, predicted_classes)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "\n",
        "    fig_save_path = os.path.join(experiment_rootdir, f\"confussion_matrix_{version}.png\")\n",
        "    plt.savefig(fig_save_path)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vxujQd_hxEu"
      },
      "source": [
        "# Training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_AbmfZWhxEv"
      },
      "source": [
        "# Create many models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSyy37I-gRn5"
      },
      "source": [
        "## VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "LV2RuydXhxEv",
        "outputId": "2c18c02a-ef16-48a0-9e42-226b2c032567"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef VGG16(img_width,img_height,img_channels):\\n\\t#  dropout rate for FC layers\\n\\tdropout=0.5\\n\\n\\t# CNN architecture\\n\\tinput_image = Input(shape=(img_width,img_height,img_channels))\\n\\tx1 = Conv2D(64, (3, 3),padding='same', activation='relu')(input_image)\\n\\tx1 = Conv2D(64, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = MaxPooling2D((2, 2))(x1)\\n\\tx1 = Conv2D(128, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(128, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = MaxPooling2D((2, 2))(x1)\\n\\tx1 = Conv2D(256, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(256, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(256, (1, 1),padding='same', activation='relu')(x1)\\n\\tx1 = MaxPooling2D((2, 2))(x1)\\n\\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(512, (1, 1),padding='same', activation='relu')(x1)\\n\\tx1 = MaxPooling2D((2, 2))(x1)\\n\\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\\n\\tx1 = Conv2D(512, (1, 1),padding='same', activation='relu')(x1)\\n\\tx1 = MaxPooling2D((2, 2))(x1)\\n\\n\\tx1 = Flatten()(x1)\\n\\n\\tx=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x1)\\n\\tx=Dropout(dropout)(x)\\n\\tx=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x)\\n\\tx=Dropout(dropout)(x)\\n\\tout= Dense(num_classes, activation='softmax')(x)\\n\\n\\tmodel = Model(inputs = input_image, outputs = out);\\n\\n\\treturn model\\n\\nmodel = VGG16(img_width,img_height,img_channels)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "'''\n",
        "def VGG16(img_width,img_height,img_channels):\n",
        "\t#  dropout rate for FC layers\n",
        "\tdropout=0.5\n",
        "\n",
        "\t# CNN architecture\n",
        "\tinput_image = Input(shape=(img_width,img_height,img_channels))\n",
        "\tx1 = Conv2D(64, (3, 3),padding='same', activation='relu')(input_image)\n",
        "\tx1 = Conv2D(64, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = MaxPooling2D((2, 2))(x1)\n",
        "\tx1 = Conv2D(128, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(128, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = MaxPooling2D((2, 2))(x1)\n",
        "\tx1 = Conv2D(256, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(256, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(256, (1, 1),padding='same', activation='relu')(x1)\n",
        "\tx1 = MaxPooling2D((2, 2))(x1)\n",
        "\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(512, (1, 1),padding='same', activation='relu')(x1)\n",
        "\tx1 = MaxPooling2D((2, 2))(x1)\n",
        "\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(512, (3, 3),padding='same', activation='relu')(x1)\n",
        "\tx1 = Conv2D(512, (1, 1),padding='same', activation='relu')(x1)\n",
        "\tx1 = MaxPooling2D((2, 2))(x1)\n",
        "\n",
        "\tx1 = Flatten()(x1)\n",
        "\n",
        "\tx=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x1)\n",
        "\tx=Dropout(dropout)(x)\n",
        "\tx=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x)\n",
        "\tx=Dropout(dropout)(x)\n",
        "\tout= Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "\tmodel = Model(inputs = input_image, outputs = out);\n",
        "\n",
        "\treturn model\n",
        "\n",
        "model = VGG16(img_width,img_height,img_channels)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e1g3e3ego7r"
      },
      "source": [
        "## RestNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "c-63WTaigsY9",
        "outputId": "ac0462e5-1500-4c4b-a94f-26fd0f991bcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndef RestNet (img_width,img_height,img_channels):\\n  dropout=0.5 # Para evitar que haya overfitting cancela algunas conexiones entre las neuronas aleatoriamente.\\n\\n  # CNN architecture\\n  input_image = Input(shape=(img_width,img_height,img_channels))\\n  x0 = Conv2D(64, (3, 3),padding='same', activation='relu')(input_image)\\n  x1 = Conv2D(64, (3, 3),padding='same', activation='relu')(x0)\\n  x1 = MaxPooling2D((2, 2))(x1)\\n  x2 = Conv2D(128, (3, 3),padding='same', activation='relu')(x1)\\n  x3 = Conv2D(128, (3, 3),padding='same')(x2)\\n  x4 = Add()([x3, x2])\\n  x4 = Activation('relu')(x4)\\n  x4 = MaxPooling2D((2, 2))(x4)\\n  x4 = Conv2D(256, (3, 3),padding='same', activation='relu')(x4)\\n  x5 = Conv2D(256, (3, 3),padding='same', activation='relu')(x4)\\n  x5 = Conv2D(256, (1, 1),padding='same')(x5)\\n  x6 = Add()([x5, x4])\\n  x6 = Activation('relu')(x6)\\n  x6 = MaxPooling2D((2, 2))(x6)\\n  x6 = Conv2D(512, (3, 3),padding='same', activation='relu')(x6)\\n  x7 = Conv2D(512, (3, 3),padding='same', activation='relu')(x6)\\n  x7 = Conv2D(512, (1, 1),padding='same')(x7)\\n  x8 = Add()([x7, x6])\\n  x8 = Activation('relu')(x8)\\n  x8 = MaxPooling2D((2, 2))(x8)\\n  x8 = Conv2D(512, (3, 3),padding='same', activation='relu')(x8)\\n  x9 = Conv2D(512, (3, 3),padding='same', activation='relu')(x8)\\n  x9 = Conv2D(512, (1, 1),padding='same')(x9)\\n  x10 = Add()([x9, x8])\\n  x10 = Activation('relu')(x10)\\n  x10 = MaxPooling2D((2, 2))(x10)\\n  x10 = Flatten()(x10)\\n\\n  x=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x10)\\n  x=Dropout(dropout)(x)\\n  x=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x)\\n  x=Dropout(dropout)(x)\\n  out= Dense(num_classes, activation='softmax')(x)\\n\\n  model = Model(inputs = input_image, outputs = out);\\n\\n  return model\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "'''\n",
        "def RestNet (img_width,img_height,img_channels):\n",
        "  dropout=0.5 # Para evitar que haya overfitting cancela algunas conexiones entre las neuronas aleatoriamente.\n",
        "\n",
        "  # CNN architecture\n",
        "  input_image = Input(shape=(img_width,img_height,img_channels))\n",
        "  x0 = Conv2D(64, (3, 3),padding='same', activation='relu')(input_image)\n",
        "  x1 = Conv2D(64, (3, 3),padding='same', activation='relu')(x0)\n",
        "  x1 = MaxPooling2D((2, 2))(x1)\n",
        "  x2 = Conv2D(128, (3, 3),padding='same', activation='relu')(x1)\n",
        "  x3 = Conv2D(128, (3, 3),padding='same')(x2)\n",
        "  x4 = Add()([x3, x2])\n",
        "  x4 = Activation('relu')(x4)\n",
        "  x4 = MaxPooling2D((2, 2))(x4)\n",
        "  x4 = Conv2D(256, (3, 3),padding='same', activation='relu')(x4)\n",
        "  x5 = Conv2D(256, (3, 3),padding='same', activation='relu')(x4)\n",
        "  x5 = Conv2D(256, (1, 1),padding='same')(x5)\n",
        "  x6 = Add()([x5, x4])\n",
        "  x6 = Activation('relu')(x6)\n",
        "  x6 = MaxPooling2D((2, 2))(x6)\n",
        "  x6 = Conv2D(512, (3, 3),padding='same', activation='relu')(x6)\n",
        "  x7 = Conv2D(512, (3, 3),padding='same', activation='relu')(x6)\n",
        "  x7 = Conv2D(512, (1, 1),padding='same')(x7)\n",
        "  x8 = Add()([x7, x6])\n",
        "  x8 = Activation('relu')(x8)\n",
        "  x8 = MaxPooling2D((2, 2))(x8)\n",
        "  x8 = Conv2D(512, (3, 3),padding='same', activation='relu')(x8)\n",
        "  x9 = Conv2D(512, (3, 3),padding='same', activation='relu')(x8)\n",
        "  x9 = Conv2D(512, (1, 1),padding='same')(x9)\n",
        "  x10 = Add()([x9, x8])\n",
        "  x10 = Activation('relu')(x10)\n",
        "  x10 = MaxPooling2D((2, 2))(x10)\n",
        "  x10 = Flatten()(x10)\n",
        "\n",
        "  x=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x10)\n",
        "  x=Dropout(dropout)(x)\n",
        "  x=Dense(4096, activation='relu', kernel_constraint=MaxNorm(3))(x)\n",
        "  x=Dropout(dropout)(x)\n",
        "  out= Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "  model = Model(inputs = input_image, outputs = out);\n",
        "\n",
        "  return model\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T715-PPGuUGu",
        "outputId": "34d45881-f62c-4093-c3af-7ea995375652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 20, 15, 512)       14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d_6  (None, 512)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14982470 (57.15 MB)\n",
            "Trainable params: 266758 (1.02 MB)\n",
            "Non-trainable params: 14715712 (56.14 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the input shape based on your image dimensions\n",
        "input_shape = (img_width, img_height, img_channels)\n",
        "\n",
        "# Load the pre-trained ResNet50 model without the top (fully connected) layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model by adding custom top layers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation=None),  # No activation here\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),  # Activation after BatchNormalization\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=initial_lr),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn9iTkKthi13"
      },
      "source": [
        "## Model execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "V9Yx3WzqgnKl",
        "outputId": "8681575f-6715-43c3-cb98-a4b543446cc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#Load parameters\\nif weights_file.is_file():\\n\\tmodel.load_weights(weights_path)\\n\\n#Model\\nmodel = RestNet(img_width,img_height,img_channels)\\n\\n# Print the architecture of the model\\nmodel.summary()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "'''\n",
        "#Load parameters\n",
        "if weights_file.is_file():\n",
        "\tmodel.load_weights(weights_path)\n",
        "\n",
        "#Model\n",
        "model = RestNet(img_width,img_height,img_channels)\n",
        "\n",
        "# Print the architecture of the model\n",
        "model.summary()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp_FzEzYhxEv"
      },
      "source": [
        "### Set model training process\n",
        "Configuration of several training decisions:\n",
        "1. Optimizer using `Adam`\n",
        "2. Model training configuration using `compile`. Use a loss and a metric function appropriate for the task\n",
        "3. Creation of the data generator for the training dataset using `image_dataset_from_directory`\n",
        "4. Creation of the data generator for the validation dataset using `image_dataset_from_directory`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FAy4roRXhxEw",
        "outputId": "f2248996-4333-41a3-887c-e2aac83e20bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Set random seed\\nif random_seed:\\n    seed = np.random.randint(0,2*31-1)\\nelse:\\n    seed = 5\\nnp.random.seed(seed)\\ntf.random.set_seed(seed)\\n\\n\\n# 1. Configure optimizer\\nadam = Adam(learning_rate=initial_lr)\\n\\n# 2. Configure training process\\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "'''\n",
        "# Set random seed\n",
        "if random_seed:\n",
        "    seed = np.random.randint(0,2*31-1)\n",
        "else:\n",
        "    seed = 5\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "# 1. Configure optimizer\n",
        "adam = Adam(learning_rate=initial_lr)\n",
        "\n",
        "# 2. Configure training process\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['categorical_accuracy'])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6qILA4XhxEw"
      },
      "source": [
        "### Train the model\n",
        "1. Load parameters from previous trainings if they exist.\n",
        "2. Fit the model\n",
        "3. Save the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqctz0VshxEw",
        "outputId": "71e658f2-9254-40dd-bf76-5ea0d13d069f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Impossible to find weight path. Returning untrained model\n",
            "Epoch 1/20\n",
            "60/60 [==============================] - 50s 784ms/step - loss: 1.9054 - accuracy: 0.3297 - val_loss: 2.1238 - val_accuracy: 0.2696\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - 47s 760ms/step - loss: 1.4043 - accuracy: 0.4671 - val_loss: 2.1513 - val_accuracy: 0.3137\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - 50s 802ms/step - loss: 1.2392 - accuracy: 0.5126 - val_loss: 2.0680 - val_accuracy: 0.3725\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - 50s 810ms/step - loss: 1.0688 - accuracy: 0.5759 - val_loss: 1.8520 - val_accuracy: 0.4167\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - 50s 813ms/step - loss: 1.0035 - accuracy: 0.6172 - val_loss: 1.4995 - val_accuracy: 0.4118\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - 47s 767ms/step - loss: 0.9054 - accuracy: 0.6560 - val_loss: 1.8646 - val_accuracy: 0.4363\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - 48s 774ms/step - loss: 0.8362 - accuracy: 0.6712 - val_loss: 1.6096 - val_accuracy: 0.4559\n",
            "Epoch 8/20\n",
            "60/60 [==============================] - 49s 802ms/step - loss: 0.7416 - accuracy: 0.7175 - val_loss: 1.7049 - val_accuracy: 0.4167\n",
            "Epoch 9/20\n",
            "32/60 [===============>..............] - ETA: 17s - loss: 0.6984 - accuracy: 0.7469"
          ]
        }
      ],
      "source": [
        "# Load pretrained model\n",
        "weights_load_path = os.path.join(experiment_rootdir, weights_path)\n",
        "if weights_load_path:\n",
        "    try:\n",
        "        model.load_weights(weights_load_path)\n",
        "        print(\"Loaded model from {}\".format(weights_load_path))\n",
        "    except:\n",
        "        print(\"Impossible to find weight path. Returning untrained model\")\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Save weights\n",
        "weights_save_path = os.path.join(experiment_rootdir, weights_path)\n",
        "model.save_weights(weights_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qesOAn11hxEw"
      },
      "source": [
        "### Plot history for accuracy and loss for the training  process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f60KHH6hxEx"
      },
      "outputs": [],
      "source": [
        "plot_accuracy_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4krKV8PShxEx"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "racjpVoMhxEy"
      },
      "source": [
        "### Testing process\n",
        "Compute the loss function and accuracy for the test data (using `evaluate`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u4LEvGDhxEy"
      },
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "scores = model.evaluate(test_ds, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "print(\"Loss: %.2f%%\" % scores[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8nKRlglhxEy"
      },
      "source": [
        "### Generation of the predictions and the confusion matrix for the test data\n",
        "1.   Compute predictions with `predict`\n",
        "2.   Select the most probable class with `argmax`\n",
        "\n",
        "The confusion matrix will be shown with `plotcm`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVDE9mRghxEy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Get predictions\n",
        "prob_class = model.predict(test_ds, batch_size=batch_size)\n",
        "\n",
        "# 2. Prediced labels\n",
        "y_pred = tf.argmax(prob_class, axis=-1)\n",
        "\n",
        "# Get ground truth\n",
        "y_true = tf.argmax(tf.concat([label for image, label in test_ds], axis=0), axis=1)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "#plotcm.plotcm(experiment_rootdir, version, y_true, y_pred, CLASSES, experiment_rootdir, normalize=True)\n",
        "plot_confusion_matrix(y_true, y_pred)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sagemaker-distribution:Python",
      "language": "python",
      "name": "conda-env-sagemaker-distribution-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}