{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vision Transformers\n",
    "The following networks will be used for musical genre classification. This is because the task of classify all the new music that is released nowadays it is impossible to be done by a human being."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7759e2e5f29586ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fdff3df3bf90f5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "from vit_keras import vit, utils, visualize\n",
    "\n",
    "# Import function to plot the results\n",
    "import plots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66ac0a4cd82a37fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data and parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d51427665edbd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data parameters and paths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4773d19f88cb067e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Randomize the initial network weights\n",
    "random_seed = True\n",
    "\n",
    "# Paths to where training, testing, and validation images are\n",
    "database_dir = 'dataset'\n",
    "train_dir = f'{database_dir}/training'\n",
    "val_dir = f'{database_dir}/val'\n",
    "test_dir = f'{database_dir}/test'\n",
    "\n",
    "# Directory where to store weights of the model and results\n",
    "root_dir = \"results\"\n",
    "# Create root directory for results if it does not exist\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "\n",
    "# Input dimension (number of subjects in our problem)\n",
    "num_classes = 6\n",
    "\n",
    "# Name of each gesture of the database\n",
    "CLASSES = [x for x in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, x))]\n",
    "print(f'The classes to classify are: {CLASSES}')\n",
    "#CLASSES = ['Alternative', 'Pop', 'Rock', 'Dance', 'Classical', 'Techno']\n",
    "\n",
    "# Parameters that characterize the images\n",
    "img_height = 369\n",
    "img_width = 496\n",
    "resize_size = 400\n",
    "img_channels = 3 # although some images could be rgb, we work with grayscale images\n",
    "color_mode = 'rgb'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e5e39548db28bea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configuration Training Parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f24de82ed75b2ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Parameters that configures the training process\n",
    "batch_size = 1  # Batch size\n",
    "epochs = 5  # Number of epochs\n",
    "initial_lr = 1e-5   # Learning rate\n",
    "seed = 42  # Random number\n",
    "modelRNN = 'ViT'  # RNN model which will be used\n",
    "version = f'{modelRNN}_BS{batch_size}_E{epochs}_LR{initial_lr}'\n",
    "experiment_dir = f'{root_dir}/{modelRNN}'\n",
    "\n",
    "# Create experiment directory if it does not exist\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "\n",
    "# Set random seed\n",
    "if random_seed:\n",
    "    seed = np.random.randint(0,2*31-1)\n",
    "else:\n",
    "    seed = 5\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dee02ebd356eced8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Load"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f47ddfcdf4ce738d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Generate train dataset (ds) from directory of samples\n",
    "train_ds = image_dataset_from_directory(directory=train_dir,\n",
    "                                        label_mode = 'categorical',\n",
    "                                        class_names=CLASSES,\n",
    "                                        batch_size=batch_size,\n",
    "                                        color_mode=color_mode,\n",
    "                                        image_size=(img_width,img_height), shuffle=True)\n",
    "\n",
    "# 2. Generate validation dataset (ds) from directory of samples\n",
    "val_ds  = image_dataset_from_directory(directory=val_dir,\n",
    "                                       label_mode = 'categorical',\n",
    "                                       class_names=CLASSES,\n",
    "                                       batch_size=batch_size,\n",
    "                                       color_mode=color_mode,\n",
    "                                       image_size=(img_width,img_height))\n",
    "\n",
    "# 3. Generate test dataset (ds) from directory of samples\n",
    "test_ds = image_dataset_from_directory(directory=test_dir,\n",
    "                                       label_mode = 'categorical',\n",
    "                                       class_names=CLASSES,\n",
    "                                       batch_size=batch_size,\n",
    "                                       color_mode=color_mode,\n",
    "                                       image_size=(img_width,img_height),\n",
    "                                       shuffle = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdca462591ec354f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing\n",
    "Because Transformers divide the images, it is better to work with square shaped images\n",
    "1. First the function with the needed transformations is defined\n",
    "2. The transformations are applied"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e82b618329f0b35a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_image(image, resize_size):\n",
    "    # Transformations which will be applied to each image in the dataset\n",
    "    resized_image = cv2.resize(image.numpy(), (resize_size, resize_size))\n",
    "    repeated_image = np.repeat(resized_image.reshape(resize_size, resize_size, 1), 3, axis=2)\n",
    "    return repeated_image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e710e3873e6ead"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1. Training dataset\n",
    "X_train = train_ds.map(lambda x, y: (preprocess_image(x, resize_size), y))\n",
    "X_train_images = np.asarray([x for x, y in X_train])\n",
    "y_train_labels = np.asarray([y for x, y in X_train])\n",
    "y_train = np_utils.to_categorical(y_train_labels, num_classes)\n",
    "\n",
    "# Validation dataset\n",
    "X_val = val_ds.map(lambda x, y: (preprocess_image(x, resize_size), y))\n",
    "X_val= np.asarray([x for x, y in X_val])\n",
    "y_val_labels = np.asarray([y for x, y in X_val])\n",
    "y_val = np_utils.to_categorical(y_val_labels, num_classes)\n",
    "\n",
    "# Test dataset\n",
    "X_test = test_ds.map(lambda x, y: (preprocess_image(x, resize_size), y))\n",
    "X_test= np.asarray([x for x, y in X_test])\n",
    "y_test_labels = np.asarray([y for x, y in X_test])\n",
    "y_test = np_utils.to_categorical(y_test_labels, num_classes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a06f8e271a746515"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-trained model\n",
    "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d893480c1c85a68a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the transformer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4028e7f4c70656d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vit_model = vit.vit_b16(\n",
    "    image_size=resize_size,\n",
    "    activation='softmax',\n",
    "    pretrained=True,\n",
    "    include_top=True,\n",
    "    pretrained_top=True\n",
    ")\n",
    "\n",
    "vit_model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ce5b623c9328c16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of Activation Map"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d629a790add27f71"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "image = X_train[300]\n",
    "\n",
    "attention_map = visualize.attention_map(model=vit_model, image=image)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(image, interpolation = 'none')\n",
    "_ = ax2.imshow(attention_map, interpolation='none')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f123cdbe9f5778b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training\n",
    "1. Model definition\n",
    "2. Model callbacks\n",
    "3. Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92b64d0c3296ccd6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for layer in vit_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    vit_model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation = tfa.activations.gelu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(num_classes, 'softmax')\n",
    "],\n",
    "    name = 'vision_transformer')\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a435b20bfb1baf25"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "362e1e4845de5454"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = initial_lr)\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
    "                                                 factor = 0.3,\n",
    "                                                 patience = 5,\n",
    "                                                 verbose = 1,\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 min_lr = 1e-6,\n",
    "                                                 mode = 'max')\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 patience = 20,\n",
    "                                                 mode = 'max',\n",
    "                                                 restore_best_weights = True,\n",
    "                                                 verbose = 1)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
    "                                                  monitor = 'val_accuracy',\n",
    "                                                  verbose = 1,\n",
    "                                                  save_best_only = True,\n",
    "                                                  save_weights_only = True,\n",
    "                                                  mode = 'max')\n",
    "\n",
    "callbacks = [earlystopping, reduce_lr, checkpointer]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90e48386994599c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "history = model.fit(x = X_train, y = y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs,verbose=1)\n",
    "\n",
    "model.save(f'{experiment_dir}/{version}.h5')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "245240494bcaae03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Results\n",
    "Accuracy and Loss obtained along the training process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51f02937f8224c5e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plots.accloss(history, modelRNN, experiment_dir, version)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4172ff89a31814e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Testing\n",
    "### Model Testing\n",
    "1. Compute the loss function and accuracy for the test data\n",
    "2. Confusion Matrix obtained from testing results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e2cef990383eae5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(test_ds, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Loss: %.2f\" % scores[0])\n",
    "\n",
    "# Obtain results to present the confusion matrix\n",
    "prob_class = model.predict(X_test, batch_size=batch_size)\n",
    "# Classified labels\n",
    "y_pred = tf.argmax(prob_class, axis=-1)\n",
    "# Visualize confusion matrix                                           \n",
    "plots.cm(y_test, y_pred, modelRNN, CLASSES, experiment_dir, version)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "233339ac91a3ef62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
